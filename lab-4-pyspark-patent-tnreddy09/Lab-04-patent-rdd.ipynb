{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 4253 / 5253 - Lab #4 - Patent Problem with Spark RDD\n",
    "<div>\n",
    " <h2> CSCI 4283 / 5253 \n",
    "  <IMG SRC=\"https://www.colorado.edu/cs/profiles/express/themes/cuspirit/logo.png\" WIDTH=50 ALIGN=\"right\"/> </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [Spark cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf) is useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import operator\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decode_citations(x):\n",
    "    list_objects = x.decode('utf-8').split(',')\n",
    "    return tuple(y.strip() for y in x.decode('utf-8').split(','))\n",
    "\n",
    "def decode_patents(x):\n",
    "    columns = x.decode('utf-8').split(',')\n",
    "    return columns[0], columns[5]\n",
    "\n",
    "def filter_null_citations(x):\n",
    "    citing, merged_tuple = x\n",
    "    cited, state = merged_tuple\n",
    "    if citing and cited and state:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def change_order(x):\n",
    "    citing, merged_tuple = x;\n",
    "    cited,state = merged_tuple\n",
    "    return cited,citing+'^'+state\n",
    "\n",
    "def filter_null_values(x):\n",
    "    a,b = x\n",
    "    if a and b:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def expand_values_mapper2(x):\n",
    "    cite, merged_tuple = x;\n",
    "    citation_str, state = merged_tuple\n",
    "    citation, citation_state = citation_str.split('^')\n",
    "    return citation, citation_state, cite, state\n",
    "\n",
    "def filter_phase2(x):\n",
    "    citing, citing_state, cited, cited_state = x\n",
    "    if citing_state == cited_state and citing_state != '\"\"' and cited_state != '\"\"\"':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def patent_file_filter(x):\n",
    "    key,value = x.decode('utf-8').split(',',1)\n",
    "    return key,value\n",
    "\n",
    "def mapper3_one(x):\n",
    "    citing, citing_state,cited,cited_state = x\n",
    "    return citing\n",
    "\n",
    "def final_result_print(x):\n",
    "    key, merged_tuple = x\n",
    "    long_str, count = merged_tuple\n",
    "    if not count:\n",
    "        count = 0\n",
    "    result_tuple = (int(key),)\n",
    "    middle_tuple = tuple(k.strip() for k in long_str.split(','))\n",
    "    result_tuple += middle_tuple\n",
    "    result_tuple += (count,)\n",
    "    return result_tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the various functions which are used in the transformation operations on the rdds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf=SparkConf().setAppName(\"Lab4-rddd\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PySpark and RDD's on the https://coding.csel.io machines is very slow -- most of the code is executed in Python and this is much less efficient than the java-based code using the PySpark dataframes. Be patient and trying using `.cache()` to cache the output of joins. You may want to start with a reduced set of data before running the full task.\n",
    "\n",
    "To that end, we've included code to just extract the last 200,000 lines of each file below using the Python \"slice\" notation. Using that subset of the data your \"new patent\" table should look like:\n",
    "\n",
    "![Top partial 10 RDD self-state citations](top-subsample-rdd.png)\n",
    "\n",
    "When you're ready to run the whole thing, just include all the data and run it again (...and wait...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two RDD's are called \"rawCitations\" and \"rawPatents\" because you probably want to process them futher (e.g. convert them to integer types, etc). If you haven't used Python \"byte\" types before, google it. You can convert a byte variable `x` into e.g. a UTF8 string using `x.decode('uft-8')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into citations rdd\n",
    "with gzip.open('cite75_99.txt.gz', 'r') as f:\n",
    "    rddCitations = sc.parallelize( f.readlines()[-800000:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into patents rdd\n",
    "with gzip.open('apat63_99.txt.gz', 'r') as f:\n",
    "    rddPatents = sc.parallelize( f.readlines()[-800000:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data into rdds before performing the transformtion operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = rddCitations.map(decode_citations)\n",
    "patent_info = rddPatents.map(decode_patents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the values are in the utf-8 encoding format, decode them and assign to a different rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map Reduce Reduce 1\n",
    "mapper1 = citations.fullOuterJoin(patent_info)\n",
    "mapper1_filter = mapper1.filter(filter_null_citations)\n",
    "mapper1 = mapper1_filter.map(change_order)\n",
    "reducer1 = mapper1.filter(filter_null_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the join on RDDs similar to dataframe join. \n",
    "Using the map operation filter the null citations.\n",
    "Change the order of the citation, cite which will be useful in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map Reduce Reduce 2\n",
    "\n",
    "mapper2 = reducer1.fullOuterJoin(patent_info)\n",
    "mapper2 = mapper2.filter(filter_null_citations)\n",
    "mapper2 = mapper2.map(expand_values_mapper2)\n",
    "reducer2 = mapper2.filter(filter_phase2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operations are similar to the steps in the Map Reduce phase 1, where we perform the join, filtering o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map Reduce Reduce 3\n",
    "\n",
    "mapper3 = reducer2.map(mapper3_one)\n",
    "reducer3 = mapper3.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5959466, '1999', '14515', '1997', '\"US\"', '\"CA\"', '5310', '2', '', '326', '4', '46', '159', '0', '1', '', '0.6186', '', '4.8868', '0.0455', '0.044', '', '', 94)\n",
      "(6008204, '1999', '14606', '1998', '\"US\"', '\"CA\"', '749584', '2', '', '514', '3', '31', '121', '0', '1', '', '0.7415', '', '5', '0.0085', '0.0083', '', '', 80)\n",
      "(5952345, '1999', '14501', '1997', '\"US\"', '\"CA\"', '749584', '2', '', '514', '3', '31', '118', '0', '1', '', '0.7442', '', '5.1102', '0', '0', '', '', 78)\n",
      "(5999972, '1999', '14585', '1996', '\"US\"', '\"CA\"', '551495', '2', '', '709', '2', '22', '352', '0', '1', '', '0.8714', '', '4.0398', '0.0117', '0.0114', '', '', 77)\n",
      "(5998655, '1999', '14585', '1998', '\"US\"', '\"CA\"', '', '1', '', '560', '1', '14', '114', '0', '1', '', '0.7387', '', '5.1667', '', '', '', '', 76)\n",
      "(5958954, '1999', '14515', '1997', '\"US\"', '\"CA\"', '749584', '2', '', '514', '3', '31', '116', '0', '1', '', '0.7397', '', '5.181', '0', '0', '', '', 76)\n",
      "(5987245, '1999', '14564', '1996', '\"US\"', '\"CA\"', '551495', '2', '', '709', '2', '22', '341', '0', '1', '', '0.8737', '', '4.0587', '0.0121', '0.0117', '', '', 76)\n",
      "(5980517, '1999', '14557', '1998', '\"US\"', '\"CA\"', '733846', '2', '', '606', '3', '32', '241', '0', '1', '', '0.7394', '', '8.3776', '0', '0', '', '', 73)\n",
      "(5951547, '1999', '14501', '1997', '\"US\"', '\"CA\"', '733846', '2', '', '606', '3', '32', '242', '0', '1', '', '0.7382', '', '8.3471', '0', '0', '', '', 73)\n",
      "(5998471, '1999', '14585', '1998', '\"US\"', '\"CA\"', '749584', '2', '', '514', '3', '31', '103', '0', '1', '', '0.7151', '', '5.5825', '0', '0', '', '', 65)\n"
     ]
    }
   ],
   "source": [
    "citation_count = sc.parallelize(reducer3.items())\n",
    "patent_rdd = rddPatents.map(patent_file_filter)\n",
    "resultant_rdd = patent_rdd.fullOuterJoin(citation_count)\n",
    "resultant_rdd = resultant_rdd.map(final_result_print)\n",
    "resultant_rdd_descending_order = resultant_rdd.takeOrdered(10, key=lambda x: -x[23])\n",
    "for element in resultant_rdd_descending_order:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
